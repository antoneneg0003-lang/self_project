## ARC-AGI Overview

![ARC-AGI Overview](https://raw.githubusercontent.com/negzero/skills/main/arc_about.png)

---

## General Intelligence Diagram

![General Intelligence Diagram](https://raw.githubusercontent.com/negzero/skills/main/general_intelligence_diagram.png)




# Creation of Problem Solving Theory (PST)

---

## OBJECTIVE

Design a structured Problem Solving Theory (PST) that:

- Develops a user capable of **broad generalization**
- Develops a user capable of **extreme generalization (ARC-level)**
- Builds a structured **Problem Solving Toolkit (PST Toolkit)**
- Designs an environment that supports abstraction and transfer
- Trains generalization under minimal information

---

## CORE PRINCIPLE

Intelligence is NOT performance on a single task.

Intelligence = Ability to generalize to unseen problems  
with minimal information  
under uncertainty  
by extracting structure.

---

## HUMAN GENERALIZATION ALGORITHM (PST MODEL)

### Stage 0: Cognitive Foundation

Before generalization is possible, strengthen:

- Attention control
- Working memory
- Emotional regulation
- Curiosity
- Uncertainty tolerance
- Cognitive endurance

Without this layer, abstraction collapses under difficulty.

---

### Stage 1: Structured Multi-Domain Exposure

Expose user to diverse structured domains:

- Math  
  - Combinatorics  
  - Number theory  
  - Probability  
  - Geometry  
  - Calculus  
  - Game theory  
- Competitive programming
- Art (structure & composition)
- Storyboarding
- Strategic interaction
- Logical reasoning

**IMPORTANT:**  
Exposure must not be passive.  
Each domain must be approached with structure extraction.

---

### Stage 2: Structure Extraction Protocol

After solving any problem, perform:

1. What was invariant?
2. What changed?
3. What constraint defined the system?
4. What transformation occurred?
5. What abstract pattern exists?
6. Where else can this pattern apply?

Compress into:

Problem → Structure → Schema → Tool

This builds abstract reasoning layers.

---

### Stage 3: Transfer Training (Cross-Domain Generalization)

Deliberately apply structure from Domain A to Domain B.

Examples:
- Combinatorics → Resource allocation
- Game theory → Social interaction
- Geometry → Data organization
- Programming → Strategic planning

Goal:  
Increase abstraction mobility.

---

### Stage 4: Minimal Information Training (ARC-like Training)

Simulate ARC conditions:

- Few examples only
- No explanation
- New type of problem
- Limited time
- No repeated exposure

Required behavior:
- Infer rule
- Form hypothesis
- Test internally
- Adapt quickly

This trains:
- Rule induction
- Uncertainty tolerance
- Sample efficiency
- Generalization efficiency

---

### Stage 5: Feedback → Reflection → Reinforcement

For each problem:

- Was abstraction correct?
- Did structure transfer?
- What assumption failed?
- Can the schema be refined?
- Can this tool be generalized further?

Update toolkit.

---

## TOOLKIT STRUCTURE

Toolkit contains:

- Thinking modes  
  - Analytical  
  - Lateral  
  - Critical  
  - Probabilistic  
  - Strategic  
  - Reflective  
  - Meta-cognitive  
- Problem solving phases
- Structure extraction protocol
- Transfer drills
- Constraint training
- Uncertainty training
- Feedback loops

Toolkit must evolve.

---

## ENVIRONMENT DESIGN

Environment should:

- Minimize shallow stimulation
- Maximize cognitive objects (structured problems)
- Encourage boredom → curiosity cycle
- Provide honest feedback
- Reward structure extraction, not just success
- Expose user to uncertainty regularly

---

## DEFINITION OF GENERAL INTELLIGENCE (PST VERSION)

General Intelligence =

Ability to:
- Extract structure from novel environments
- Learn from minimal examples
- Transfer abstractions across domains
- Operate under uncertainty
- Adapt quickly without memorization

---

## FINAL MODEL SUMMARY

Cognitive Foundation  
↓  
Structured Exposure  
↓  
Structure Extraction  
↓  
Abstraction Compression  
↓  
Transfer Training  
↓  
Minimal Information Challenges  
↓  
Reflection & Reinforcement  
↓  
Improved Generalization Ability

---

## IMPORTANT CLARIFICATION

General intelligence does NOT emerge from:
- Studying many subjects alone
- Memorizing tools
- Performing repetitive tasks

It emerges from:  
Repeated abstraction + transfer + uncertainty training.

---

# CORE DEFINITIONS FOR ARC-LIKE TRAINING

## REQUIRED BEHAVIOR

### 1. Infer Rule

#### Definition

Infer rule =  
To deduce the hidden transformation or governing principle  
that explains how inputs become outputs  
based on limited examples.

It is extracting the underlying mapping.

#### Example

Given:

2 → 5  
4 → 9  
6 → 13  

You infer:

Rule = (Input × 2) + 1

You were not told the formula.  
You derived it from pattern.

---

### 2. Form Hypothesis

#### Definition

Form hypothesis =  
To propose a candidate rule that might explain the pattern,  
even if it is not yet confirmed.

A hypothesis is a testable explanation.

#### Example

From:

Red square → Blue square  
Red circle → Blue circle  

Hypothesis:

"All red objects become blue."

It might be wrong.  
But it is structured and testable.

---

### 3. Test Internally

#### Definition

Test internally =  
To mentally simulate the hypothesis  
against known examples  
to check consistency before acting.

It is internal verification.

#### Example

Hypothesis:

"Output = input × 2 + 1"

Test with example:

4 → 9  
4 × 2 + 1 = 9 ✔

If mismatch → reject or modify hypothesis.

---

### 4. Adapt Quickly

#### Definition

Adapt quickly =  
To update or replace a failed hypothesis  
without emotional delay or rigid attachment.

It is flexible model updating.

#### Example

Hypothesis:

"All red objects become blue."

New example:

Red triangle → Green triangle  

Your hypothesis fails.

Adapt:

"Maybe only red circles and squares become blue."

Update model immediately.

---

## WHAT THIS TRAINS

### 1. Rule Induction

#### Definition

Ability to discover general rules  
from specific examples.

Example:  
Seeing multiple math problems  
and extracting a reusable formula.

---

### 2. Uncertainty Tolerance

#### Definition

Ability to function and reason  
without full information  
without emotional panic.

Example:  
Solving a puzzle with incomplete data  
without needing immediate clarity.

---

### 3. Sample Efficiency

#### Definition

Ability to learn accurate rules  
from very few examples.

High sample efficiency = Learn from 2–3 examples.  
Low sample efficiency = Need 100 examples.

---

### 4. Generalization Efficiency

#### Definition

Ability to correctly apply an inferred rule  
to new unseen situations  
with minimal retraining.

Example:  
Learning structure in geometry  
and applying similar reasoning in programming.

---

# STAGE 2: STRUCTURE EXTRACTION LAYER

## Purpose

Stage 2 exists to prevent domain-specific memorization.

Its purpose is to convert solved problems into abstract, transferable cognitive structures.

Without this stage:
- User becomes skilled in a domain.
- But cannot generalize across domains.

With this stage:
- User builds abstraction.
- Abstraction enables transfer.
- Transfer enables generalization.

---

## Core Principle

Every solved problem must be compressed into structure.

Not:

"How did I solve this specific problem?"

But:

"What structure made this solvable?"

---

## Structure Extraction Protocol

### Step 1: Identify the Invariant

What stayed constant across examples?

Examples:
- Constraint
- Rule type
- Relationship
- Goal condition

Invariant = structural backbone.

---

### Step 2: Identify the Variable Elements

What changed?

Examples:
- Numbers
- Objects
- Positions
- Input format

This separates surface detail from deep structure.

---

### Step 3: Identify the Constraint System

What limited possible outcomes?

Examples:
- Conservation law
- Resource limitation
- Logical rule
- Symmetry
- Optimization requirement

Constraints define the problem’s geometry.

---

### Step 4: Identify the Transformation

What mapping occurred from input → output?

Examples:
- Rotate
- Reflect
- Reorder
- Multiply
- Eliminate
- Reallocate

Transformation = rule mechanism.

---

### Step 5: Abstract the Pattern

Remove domain labels.

Convert:

"Triangle rotation problem"

Into:

"Spatial transformation under fixed constraint"

Convert:

"Distribution of coins problem"

Into:

"Discrete allocation under conservation constraint"

Abstraction removes context.  
Only structure remains.

---

### Step 6: Compress into Schema (Tool Creation)

Store as:

Problem Type →  
Core Structure →  
Applicable Domains →  
Failure Conditions

Example:

Schema:  
Name: Resource Allocation Under Constraint  
Structure: Fixed total, multiple agents, optimization goal  
Transfer Domains: Game theory, budgeting, scheduling, CPU allocation  

---

## Output of Stage 2

Repeated execution of Stage 2 builds:

- Structural pattern library
- Abstraction compression ability
- Transfer potential
- Reduced surface distraction
- Increased generalization power

---

## What Stage 2 Is NOT

- It is not repetition.
- It is not memorizing steps.
- It is not solving more problems.

It is converting solved problems into reusable structure.

---

## Why This Builds Generalization

Generalization occurs when:

New problem →  
User detects similar structure →  
Applies stored schema →  
Adapts to new surface details.

Without structure extraction:  
Each new problem feels new.

With structure extraction:  
Many problems collapse into same category.

---

# What Is "Structure" and "Schema" in a Problem?

## 1️⃣ Surface Characteristics

Surface characteristics are visible features of a problem.

They describe what the problem looks like.

Examples:
- Numbers involved
- Shapes used
- Topic name (geometry, algebra, etc.)
- Story context
- Symbols used
- Difficulty level

Surface answers:

"What does the problem look like?"

Surface does NOT define deep logic.

---

## 2️⃣ Structure (Precise Definition)

Structure =

The underlying relationships, constraints, and transformation rules  
that determine how elements of the problem interact.

Structure answers:
- What depends on what?
- What is invariant?
- What changes?
- What is conserved?
- What constraint limits the system?
- What transformation maps input → output?

Structure is the relational skeleton of the problem.

It remains meaningful when surface details are removed.

Structure answers:

"How does the system behave?"

---

### Example 1

Problem A:  
Distribute 10 coins among 3 people.

Problem B:  
Assign 10 tasks to 3 machines.

Surface:  
Coins vs tasks.  
People vs machines.

Structure:
- Fixed total resource
- Multiple agents
- Allocation constraint

Same structure.  
Different surface.

---

### Example 2

Geometry:  
Rotate a triangle 90°.

Programming:  
Shift array elements by 1 position.

Surface:  
Shapes vs arrays.

Structure:  
Element re-mapping within an ordered system.

Same transformation pattern.  
Different domain.

---

### Example 3 (ARC-like)

Input:  
Red square → Blue square

Surface:  
Color and shape.

Structure:  
Attribute substitution while preserving identity.

Invariant:  
Shape

Transformation:  
Color change

---

## 3️⃣ Schema (Precise Definition)

Schema =

A compressed mental representation of a structure  
that can be reused across different problems.

It is a stored structural pattern.

Structure is discovered.  
Schema is stored.

---

## Difference Between Structure and Schema

Structure =  
The underlying relational logic of one problem.

Schema =  
A reusable template extracted from multiple similar structures.

Structure is specific.  
Schema is generalized and reusable.

---

## Example of Schema Creation

After solving multiple problems like:

- Distributing coins
- Assigning tasks
- Allocating time

You extract the common structure:

Fixed total + Multiple agents + Constraint

You compress it into a schema:

Schema Name:  
Resource Allocation Under Constraint

Now when you see a new problem,  
your brain recognizes the pattern quickly.

That is schema activation.

---

## One-Line Definitions

Structure =  
The rule-governed relationships inside a problem.

Schema =  
A reusable mental template built from repeated structures.  
that templace is rep

---

## Why Schema Matters for Generalization

Without schema:  
Every new problem feels unrelated.

With schema:  
New problems trigger stored structural templates.

Generalization happens when:

New surface → Recognized structure → Activated schema → Adapted solution.

---

# Structure and Schema (Complete Definition)

## 1️⃣ Structure

Structure =

The underlying relationships, constraints, and transformation rules  
that determine how elements of a problem interact.

Structure answers:
- What depends on what?
- What is invariant?
- What changes?
- What constraint limits the system?
- What transformation occurs?

Structure is the relational skeleton of a problem.

---

## 2️⃣ Schema

Schema =

A compressed, reusable mental template  
that represents a recurring structure  
and guides how to solve similar problems.

Structure is discovered in one problem.  
Schema is extracted and stored for reuse.

---

## What a Schema Contains

A complete schema contains:

1. Core Structure  
   - The abstract relational pattern  
   - (e.g., Fixed total + Multiple agents + Allocation constraint)

2. Invariants  
   - What must remain constant  
   - (e.g., total resource conserved)

3. Variables  
   - What can change  
   - (e.g., number of agents, distribution amounts)

4. Constraints  
   - Rules limiting possible outcomes  
   - (e.g., no negative allocation, must use full total)

5. Transformation Rule (if applicable)  
   - How input becomes output  
   - (e.g., redistribute while maximizing fairness)

6. Typical Goal Pattern  
   - What the problem usually asks  
   - (maximize, minimize, balance, detect pattern, etc.)

7. Applicability Domains  
   - Where this structure appears  
   - (math, programming, economics, scheduling)

8. Failure Conditions  
   - When the schema does NOT apply  
   - (e.g., resource not fixed → different structure)

---

## Example Schema

Schema Name:  
Resource Allocation Under Constraint

Contains:

Core Structure:  
Fixed total resource distributed among multiple agents.

Invariants:  
Total resource remains constant.

Variables:  
Number of agents, allocation amount.

Constraints:  
Non-negative allocation, must sum to total.

Typical Goals:  
Maximize efficiency, minimize imbalance, optimize fairness.

Domains:  
Combinatorics, budgeting, CPU scheduling, game theory.

Failure Condition:  
If total resource changes dynamically, this schema is invalid.

---

## One-Line Definition

Schema =  
A stored structural template containing  
core relationships, constraints, transformation rules,  
and application boundaries.

---

## Why This Matters

Structure helps you understand one problem.

Schema helps you solve many problems.

Generalization occurs when:

New problem → Recognize structure → Activate schema → Adapt solution.
